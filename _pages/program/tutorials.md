---
title: Tutorials
layout: single
excerpt: "ACL 2024 Conference Overview."
permalink: /program/tutorials/
sidebar: 
    nav: "program"
---

## Sunday, August 11th:
<style>
.news-table tr td:nth-child(1) { font-weight: bold; width: 20em; }
.news-table tr td:nth-child(2) {width: 55em; }
</style>
| 09:00 - 12:30 | Tutorial 1 - 3
{: .news-table}
* **Tutorial 1– Computational Linguistics for Brain Encoding and Decoding: Principles, Practices and Beyond** <br> <small>Room : Lotus 1-4 (Level 22) </small><br>
*Jingyuan Sun, and Shaonan Wang, and Zijiao Chen, and Jixing Li, and Marie-Francine Moens*
* **Tutorial 2 - Automatic and Human-AI Interactive Text Generation (with a focus on Text Simplification and Revision)** <br> <small>Room :  Lotus 5-7 (Level 22)</small><br>
*Yao Dou, and Philippe Laban, and Claire Gardent, and Wei Xu*
* **Tutorial 3 - Vulnerabilities of Large Language Models to Adversarial Attacks** <br> <small>Room : World Ballroom B (Level 23)</small><br>
*Yu Fu, Erfan Shayegan, and Md. Mamun Al Abdullah, and Pedram Zaree, and Nael Abu-Ghazaleh, and Yue Dong*

<style>
.news-table tr td:nth-child(1) { font-weight: bold; width: 20em; }
.news-table tr td:nth-child(2) {width: 55em; }
</style>
| 14:00 - 17:30 | Tutorial 4 - 6
{: .news-table}
* **Tutorial 4 – Computational Expressivity of Neural Language Models** <br> <small>Room: Lotus 1-4 (Level 22) </small><br>
*Alexandra Butoi and Ryan Cotterell and Anej Svete*
* **Tutorial 5– Watermarking for Large Language Model** <br> <small>Room: Lotus 5-7 (Level 22)</small><br>
*Xuandong Zhao, and Yu-Xiang Wang, and Lei Li*
* **Tutorial 6; Presentation Matters: How to Communicate Science in the NLP Venues and in the Wild?** <br> <small>Room: World Ballroom B (Level 23)</small><br>
*Sarvnaz Karimi, and Cecile Paris, and Gholamreza Haffari*

<h2>List of Tutorials</h2>
The following tutorials have been accepted for ACL 2024. <br>

**Computational Linguistics for Brain Encoding and Decoding: Principles, Practices and Beyond**<br>
  <i>Jingyuan Sun, Shaonan Wang, Zijiao Chen, Jixing Li and Marie-Francine Moens </i>
* <small>Computational linguistics (CL) has witnessed tremendous advancements in recent years, with models such as large language models demonstrating exceptional performance in various natural language processing tasks. These advancements highlight their potential to help understand brain language processing, especially through the lens of brain encoding and decoding.
<br>Brain encoding involves the mapping of linguistic stimuli to brain activity, while brain decoding is the process of reconstructing linguistic stimuli from observed brain activities. CL models that excel at capturing and manipulating linguistic features are crucial for mapping linguistic stimuli to brain activities and vice versa. Brain encoding and decoding have vast applications, from enhancing human-computer interaction to developing assistive technologies for individuals with communication impairments.
<br>This tutorial will focus on elucidating how computational linguistics can facilitate brain encoding and decoding. We will delve into the principles and practices of using computational linguistics methods for brain encoding and decoding. We will also discuss the challenges and future directions of brain encoding and decoding. Through this tutorial, we aim to provide a comprehensive and informative overview of the intersection between computational linguistics and cognitive neuroscience, inspiring future research in this exciting and rapidly evolving field. </small>

**Automatic and Human-AI Interactive Text Generation (with a focus on Text Simplification and Revision)** <br>
  <i>Yao Dou, Philippe Laban, Claire Gardent and Wei Xu</i>
* <small>In this tutorial, we focus on text-to-text generation, a class of natural language generation (NLG) tasks, that takes a piece of text as input and then generates a revision that is improved according to some specific criteria (e.g., readability or linguistic styles), while largely retaining the original meaning and the length of the text. This includes many useful applications, such as text simplification, paraphrase generation, style transfer, etc. In contrast to text summarization and open-ended text completion (e.g., story), the text-to-text generation tasks we discuss in this tutorial are more constrained in terms of semantic consistency and targeted language styles. This level of control makes these tasks ideal testbeds for studying the ability of models to generate text that is both semantically adequate and stylistically appropriate. Moreover, these tasks are interesting from a technical standpoint, as they require complex combinations of lexical and syntactical transformations, stylistic control, and adherence to factual knowledge, -- all at once. With a special focus on text simplification and revision, this tutorial aims to provide an overview of the state-of-the-art natural language generation research from four major aspects -- Data, Models, Human-AI Collaboration, and Evaluation -- and to discuss and showcase a few significant and recent advances: (1) the use of non-retrogressive approaches; (2) the shift from fine-tuning to prompting with large language models; (3) the development of new learnable metric and fine-grained human evaluation framework; (4) a growing body of studies and datasets on non-English languages; (5) the rise of HCI+NLP+Accessibility interdisciplinary research to create real-world writing assistant systems.</small>

**Computational Expressivity of Neural Language Models**<br>
  <i>Alexandra Butoi, Ryan Cotterell and Anej Svete</i>
* <small>Language models (LMs) are currently at the forefront of NLP research due to their remarkable versatility across diverse tasks. However, a large gap exists between their observed capabilities and the explanations proposed by established formal machinery. To motivate a better theoretical characterization of LMs' abilities and limitations, this tutorial aims to provide a comprehensive introduction to a specific framework for formal analysis of modern LMs using tools from formal language theory (FLT). We present how tools from FLT can be useful in understanding the inner workings and predicting the capabilities of modern neural LM architectures. We will cover recent results using FLT to make precise and practically relevant statements about LMs based on recurrent neural networks and transformers by relating them to formal devices such as finite-state automata, Turing machines, and analog circuits. Altogether, the results covered in this tutorial will allow us to make precise statements and explanations about the observed as well as predicted behaviors of LMs, as well as provide theoretically motivated suggestions on the aspects of the architectures that could be improved.</small>

**Presentation Matters: How to Communicate Science in the NLP Venues and in the Wild?**<br>
  <i>Sarvnaz Karimi, Cecile Paris and Gholamreza Haffari</i>
* <small>Each year a large number of early career researchers join the NLP/Computational Linguistics community, with most starting by presenting their research in the *ACL conferences and workshops. While writing a paper that has made it to these venues is one important step, what comes with communicating the outcome is equally important and sets the path to impact of a research outcome. In addition, not all PhD candidates get the chance of being trained for their presentation skills. Research methods courses are not all of the same quality and may not cover scientific communications, and certainly not all are tailored to the NLP community. We are proposing an introductory tutorial that covers a range of different communication skills, including writing, oral presentation (posters and demos), and social media presence. This is to fill in the gap for the researchers who may not have access to research methods courses or other mentors who could help them acquire such skills. The interactive nature of such a tutorial would allow attendees to ask questions and clarifications which would not be possible from reading materials alone.</small>

**Vulnerabilities of Large Language Models to Adversarial Attacks**<br>
  <i>Yu Fu, Erfan Shayegan, Md. Mamun Al Abdullah, Pedram Zaree, Nael Abu-Ghazaleh and Yue Dong</i>
* <small>This tutorial serves as a comprehensive guide on the vulnerabilities of Large Language Models (LLMs) to adversarial attacks, an interdisciplinary field that blends perspectives from Natural Language Processing (NLP) and Cybersecurity. As LLMs become more complex and integrated into various systems, understanding their security attributes is crucial. However, current research indicates that even safety-aligned models are not impervious to adversarial attacks that can result in incorrect or harmful outputs. The tutorial first lays the foundation by explaining safety-aligned LLMs and concepts in cybersecurity. It then categorizes existing research based on different types of learning architectures and attack methods. We highlight the existing vulnerabilities of unimodal LLMs, multi-modal LLMs, and systems that integrate LLMs, focusing on adversarial attacks designed to exploit weaknesses and mislead AI systems. Finally, the tutorial delves into the potential causes of these vulnerabilities and discusses potential defense mechanisms.</small>

**Watermarking for Large Language Model**<br>
  <i>Xuandong Zhao, Yu-Xiang Wang and Lei Li</i>
* <small>As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial in both the computational linguistics and machine learning communities. In this tutorial, we aim to provide an in-depth exploration of text watermarking, a subfield of linguistic steganography with the goal of embedding a hidden message (the watermark) within a text passage. We will introduce the fundamentals of text watermarking, discuss the main challenges in identifying AI-generated text, and delve into the current watermarking methods, assessing their strengths and weaknesses. Moreover, we will explore other possible applications of text watermarking and discuss future directions for this field. Each section will be supplemented with examples and key takeaways.</small>






